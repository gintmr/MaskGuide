# Distill-SAM: Knowledge Distillation for Segment Anything Model

A comprehensive framework for knowledge distillation of the Segment Anything Model (SAM) to create lightweight, efficient versions suitable for resource-constrained environments.

## Overview

This project implements a knowledge distillation framework for the Segment Anything Model (SAM) to create efficient variants like Tiny MobileSAM. The framework supports various distillation strategies, multiple datasets, and provides tools for fine-tuning and evaluation.

## Features

- **Knowledge Distillation Framework**: Efficient teacher-student model distillation
- **Multi-Dataset Support**: MIMC, UIIS, COCO, VOC datasets
- **Flexible Architecture**: Support for ViT-H, ViT-T, Tiny MobileSAM, and Micro SAM
- **Comprehensive Training Pipeline**: PyTorch Lightning-based training
- **Data Processing Tools**: Dataset conversion and preprocessing utilities
- **Evaluation Metrics**: IoU, PA-IoU and other segmentation metrics

## Project Structure

```
Distill-SAM/
├── distill_tiny_msam.py              # Main training script
├── install.sh                        # Installation script
├── Datasets/                         # Dataset loading modules
│   └── coco.py                       # COCO dataset handler
├── DistillFinetune/                  # Distillation modules
│   ├── Imgencoder_Distill.py         # Image encoder distillation
│   ├── DistillFintuner.py            # Base distillation class
│   └── Data_Argument/                # Data augmentation
├── Tools_configs/                    # Configuration files
│   └── only_distill.yaml             # Training configuration
├── Tools_datasets/                   # Dataset processing tools
│   ├── modify_MIMC.py                # MIMC format converter
│   ├── Check_dataset_quality.py      # Dataset quality checker
│   └── transfer_img_style.py         # Image style transfer
├── Tools_finetune/                   # Fine-tuning utilities
│   ├── finetuner.py                  # Fine-tuning logic
│   └── loss_calculate.py             # Loss calculation
├── Tools_metrics/                    # Evaluation metrics
│   ├── eval.py                       # Evaluation script
│   └── test_performance.py           # Performance testing
├── Tools_weights/                    # Weight management
│   ├── prune_init_weights/           # Weight pruning tools
│   └── trans_ckpt.py                 # Checkpoint transformation
└── MobileSAMv2/                      # MobileSAM implementation
```

## Installation

```bash
# Clone the repository
git clone <repository-url>
cd Distill-SAM

# Install dependencies
bash install.sh

# Or manually install
pip install -r requirements.txt
```

## Quick Start

### 1. Prepare Datasets

The framework supports multiple datasets. Ensure your datasets are in COCO format:

```bash
# Convert MIMC dataset format if needed
python Tools_datasets/modify_MIMC.py
```

### 2. Configure Training Parameters

Edit the configuration in `distill_tiny_msam.py` or use the YAML config:

```bash
# Set paths to your datasets
--train_data_IMC /path/to/your/data
--train_anno_IMC /path/to/your/annotations
```

### 3. Run Training

```bash
python distill_tiny_msam.py \
    --T_model vit_t \
    --S_model tiny_msam \
    --T_checkpoint_path /path/to/teacher/checkpoint \
    --S_checkpoint_path /path/to/student/checkpoint \
    --batch_size 8 \
    --epochs 20 \
    --learning_rate 5.0e-4
```

## Supported Models

- **Teacher Models**: ViT-H, ViT-T, MobileSAM
- **Student Models**: Tiny MobileSAM, Micro SAM
- **Architecture**: Vision Transformer-based segmentation

## Training Options

### Distillation Targets
- `Img_Encoder`: Image Encoder distillation
- `Mask_Decoder`: Mask Decoder distillation
- `Prompt_Encoder`: Prompt Encoder distillation

### Distillation Modes
- `only_distill`: Only perform distillation
- `add_distill`: Add distillation to regular training
- `mask&unmask`: Combined mask and unmask distillation

### Environment Variables
- `distill`: Distillation mode ("mask&unmask_v1", "ori")
- `INFERENCE_MODE`: Inference mode ("test", "train")
- `MODEL_MODE`: Model mode ("test")
- `test_prompts`: Prompt type ("bbox", "point")

## Dataset Support

### Current Supported Datasets
- **MIMC**: Marine Image Multi-classification
- **UIIS**: Underwater Image Segmentation
- **COCO**: Common Objects in Context
- **VOC**: PASCAL Visual Object Classes

### Data Format Requirements
Datasets must be in COCO format with the following structure:

```json
{
  "images": [...],
  "categories": [...],
  "annotations": [...]
}
```

## Key Components

### 1. Distillation Framework
The `Imgencoder_Distill` class implements the knowledge distillation process with:
- Feature alignment losses
- Attention map matching
- Teacher-student feature distillation

### 2. Data Loading
- Custom COCO dataset loader
- Multi-dataset concatenation
- Weighted sampling support
- Batch processing with various prompts

### 3. Training Pipeline
- PyTorch Lightning integration
- Automatic mixed precision
- Multi-GPU support
- Checkpoint saving and loading

### 4. Evaluation Metrics
- Pixel Accuracy IoU (PA-IoU)
- Segmentation losses
- Visualization tools

## Configuration

Training parameters can be configured through:

1. **Command Line Arguments** in `distill_tiny_msam.py`
2. **YAML Configurations** in `Tools_configs/`
3. **Environment Variables**

Key parameters include:
- `--batch_size`: Training batch size
- `--image_size`: Input image size (default: 1024)
- `--learning_rate`: Learning rate (default: 5.0e-4)
- `--epochs`: Number of training epochs
- `--num_points`: Number of random points for supervision
- `--length`: Number of masks to process

## Advanced Usage

### Multi-GPU Training
Set the GPU devices using environment variables:

```bash
export CUDA_VISIBLE_DEVICES='0,1,2,3'
export NCCL_P2P_DISABLE='1'
```

### Mixed Precision Training
The framework supports automatic mixed precision for faster training:

```python
with autocast():
    # Forward pass
    loss = model(batch)
```

### Custom Loss Functions
The `OceanSegmentationLoss` class provides custom loss calculations:
- Dice loss
- Focal loss
- IoU loss
- Distillation losses

## Evaluation

Run evaluation on trained models:

```bash
python Tools_metrics/eval.py --checkpoint_path /path/to/model
```

## Results

The framework tracks various metrics during training:
- Training/validation loss
- PA-IoU scores
- Learning rate scheduling
- Model checkpoints

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Based on the Segment Anything Model (SAM) architecture
- Uses PyTorch Lightning for training infrastructure
- Inspired by MobileSAM and TinySAM implementations

## Citation

If you use this code in your research, please cite the original SAM paper and any related work.# Distill-SAM: Knowledge Distillation for Segment Anything Model

A comprehensive framework for knowledge distillation of the Segment Anything Model (SAM) to create lightweight, efficient versions suitable for resource-constrained environments.

## Overview

This project implements a knowledge distillation framework for the Segment Anything Model (SAM) to create efficient variants like Tiny MobileSAM. The framework supports various distillation strategies, multiple datasets, and provides tools for fine-tuning and evaluation.

## Features

- **Knowledge Distillation Framework**: Efficient teacher-student model distillation
- **Multi-Dataset Support**: MIMC, UIIS, COCO, VOC datasets
- **Flexible Architecture**: Support for ViT-H, ViT-T, Tiny MobileSAM, and Micro SAM
- **Comprehensive Training Pipeline**: PyTorch Lightning-based training
- **Data Processing Tools**: Dataset conversion and preprocessing utilities
- **Evaluation Metrics**: IoU, PA-IoU and other segmentation metrics

## Project Structure

```
Distill-SAM/
├── distill_tiny_msam.py              # Main training script
├── install.sh                        # Installation script
├── Datasets/                         # Dataset loading modules
│   └── coco.py                       # COCO dataset handler
├── DistillFinetune/                  # Distillation modules
│   ├── Imgencoder_Distill.py         # Image encoder distillation
│   ├── DistillFintuner.py            # Base distillation class
│   └── Data_Argument/                # Data augmentation
├── Tools_configs/                    # Configuration files
│   └── only_distill.yaml             # Training configuration
├── Tools_datasets/                   # Dataset processing tools
│   ├── modify_MIMC.py                # MIMC format converter
│   ├── Check_dataset_quality.py      # Dataset quality checker
│   └── transfer_img_style.py         # Image style transfer
├── Tools_finetune/                   # Fine-tuning utilities
│   ├── finetuner.py                  # Fine-tuning logic
│   └── loss_calculate.py             # Loss calculation
├── Tools_metrics/                    # Evaluation metrics
│   ├── eval.py                       # Evaluation script
│   └── test_performance.py           # Performance testing
├── Tools_weights/                    # Weight management
│   ├── prune_init_weights/           # Weight pruning tools
│   └── trans_ckpt.py                 # Checkpoint transformation
└── MobileSAMv2/                      # MobileSAM implementation
```

## Installation

```bash
# Clone the repository
git clone <repository-url>
cd Distill-SAM

# Install dependencies
bash install.sh

# Or manually install
pip install -r requirements.txt
```

## Quick Start

### 1. Prepare Datasets

The framework supports multiple datasets. Ensure your datasets are in COCO format:

```bash
# Convert MIMC dataset format if needed
python Tools_datasets/modify_MIMC.py
```

### 2. Configure Training Parameters

Edit the configuration in `distill_tiny_msam.py` or use the YAML config:

```bash
# Set paths to your datasets
--train_data_IMC /path/to/your/data
--train_anno_IMC /path/to/your/annotations
```

### 3. Run Training

```bash
python distill_tiny_msam.py \
    --T_model vit_t \
    --S_model tiny_msam \
    --T_checkpoint_path /path/to/teacher/checkpoint \
    --S_checkpoint_path /path/to/student/checkpoint \
    --batch_size 8 \
    --epochs 20 \
    --learning_rate 5.0e-4
```

## Supported Models

- **Teacher Models**: ViT-H, ViT-T, MobileSAM
- **Student Models**: Tiny MobileSAM, Micro SAM
- **Architecture**: Vision Transformer-based segmentation

## Training Options

### Distillation Targets
- `Img_Encoder`: Image Encoder distillation
- `Mask_Decoder`: Mask Decoder distillation
- `Prompt_Encoder`: Prompt Encoder distillation

### Distillation Modes
- `only_distill`: Only perform distillation
- `add_distill`: Add distillation to regular training
- `mask&unmask`: Combined mask and unmask distillation

### Environment Variables
- `distill`: Distillation mode ("mask&unmask_v1", "ori")
- `INFERENCE_MODE`: Inference mode ("test", "train")
- `MODEL_MODE`: Model mode ("test")
- `test_prompts`: Prompt type ("bbox", "point")

## Dataset Support

### Current Supported Datasets
- **MIMC**: Marine Image Multi-classification
- **UIIS**: Underwater Image Segmentation
- **COCO**: Common Objects in Context
- **VOC**: PASCAL Visual Object Classes

### Data Format Requirements
Datasets must be in COCO format with the following structure:

```json
{
  "images": [...],
  "categories": [...],
  "annotations": [...]
}
```

## Key Components

### 1. Distillation Framework
The `Imgencoder_Distill` class implements the knowledge distillation process with:
- Feature alignment losses
- Attention map matching
- Teacher-student feature distillation

### 2. Data Loading
- Custom COCO dataset loader
- Multi-dataset concatenation
- Weighted sampling support
- Batch processing with various prompts

### 3. Training Pipeline
- PyTorch Lightning integration
- Automatic mixed precision
- Multi-GPU support
- Checkpoint saving and loading

### 4. Evaluation Metrics
- Pixel Accuracy IoU (PA-IoU)
- Segmentation losses
- Visualization tools

## Configuration

Training parameters can be configured through:

1. **Command Line Arguments** in `distill_tiny_msam.py`
2. **YAML Configurations** in `Tools_configs/`
3. **Environment Variables**

Key parameters include:
- `--batch_size`: Training batch size
- `--image_size`: Input image size (default: 1024)
- `--learning_rate`: Learning rate (default: 5.0e-4)
- `--epochs`: Number of training epochs
- `--num_points`: Number of random points for supervision
- `--length`: Number of masks to process

## Advanced Usage

### Multi-GPU Training
Set the GPU devices using environment variables:

```bash
export CUDA_VISIBLE_DEVICES='0,1,2,3'
export NCCL_P2P_DISABLE='1'
```

### Mixed Precision Training
The framework supports automatic mixed precision for faster training:

```python
with autocast():
    # Forward pass
    loss = model(batch)
```

### Custom Loss Functions
The `OceanSegmentationLoss` class provides custom loss calculations:
- Dice loss
- Focal loss
- IoU loss
- Distillation losses

## Evaluation

Run evaluation on trained models:

```bash
python Tools_metrics/eval.py --checkpoint_path /path/to/model
```

## Results

The framework tracks various metrics during training:
- Training/validation loss
- PA-IoU scores
- Learning rate scheduling
- Model checkpoints

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Based on the Segment Anything Model (SAM) architecture
- Uses PyTorch Lightning for training infrastructure
- Inspired by MobileSAM and TinySAM implementations

## Citation

If you use this code in your research, please cite the original SAM paper and any related work.

---
Marks:
- 修改了环境变量MODEL_MODE之后，一定要注意！

    > 最好置为test。因为若置为train的话，vit_t的prompt_encoder & mask_decoder将会被随机初始化

